#!/usr/bin/env python3
"""
ì—…ë¹„íŠ¸ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì§ì ‘ ì—…ë°ì´íŠ¸
DB ì €ì¥ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ë°”ë¡œ JSON íŒŒì¼ì— ì €ì¥

âœ¨ ê°œì„ ì‚¬í•­:
- upsert: ê°™ì€ ë‚ ì§œ ë°ì´í„° ë®ì–´ì“°ê¸°
- ë³‘ë ¬ì²˜ë¦¬: ThreadPoolExecutorë¡œ 10ê°œ ë™ì‹œ ì‹¤í–‰
- ì¬ì‹œë„: API ì‹¤íŒ¨ ì‹œ 4íšŒ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)
"""

import os
import requests
import json
import time
from datetime import datetime, timedelta
from utils_common import retry_on_failure, parallel_process, upsert_history, calculate_and_update_indicators

UPBIT_API_BASE = "https://api.upbit.com/v1"
JSON_FILE_PATH = "src/data/momentum/upbit/upbit_historical_data.json"
TICKERS_FILE_PATH = "src/data/momentum/upbit/upbit_tickers.json"

# âœ… ì—…ë¹„íŠ¸ ë§ˆì¼“ ë¦¬ìŠ¤íŠ¸ (ì¬ì‹œë„ ì ìš©)
@retry_on_failure(max_retries=4)
def get_krw_markets():
    url = f"{UPBIT_API_BASE}/market/all"
    res = requests.get(url)
    res.raise_for_status()
    return [m for m in res.json() if m["market"].startswith("KRW-")]

# âœ… ì–´ì œ ì¢…ê°€ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ì ìš©)
@retry_on_failure(max_retries=4)
def fetch_yesterday_candle(market):
    kst_now = datetime.utcnow() + timedelta(hours=9)
    kst_yesterday_end = (kst_now - timedelta(days=1)).strftime("%Y-%m-%dT23:59:59")
    url = f"{UPBIT_API_BASE}/candles/days"
    params = {"market": market, "count": 1, "to": kst_yesterday_end}
    res = requests.get(url, params=params)
    res.raise_for_status()
    time.sleep(0.15)  # API rate limit (ì¦ê°€)
    return res.json()[0]

# âœ… JSON íŒŒì¼ ë¡œë“œ
def load_json_data():
    if not os.path.exists(JSON_FILE_PATH):
        return {
            'generated_at': datetime.now().isoformat(),
            'cutoff_date': (datetime.now() - timedelta(days=730)).strftime("%Y-%m-%d"),
            'total_tickers': 0,
            'total_records': 0,
            'data': []
        }

    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

# âœ… JSON íŒŒì¼ ì €ì¥
def save_json_data(data):
    os.makedirs(os.path.dirname(JSON_FILE_PATH), exist_ok=True)
    with open(JSON_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# âœ… í‹°ì»¤ ëª©ë¡ ì €ì¥
def save_tickers(tickers):
    os.makedirs(os.path.dirname(TICKERS_FILE_PATH), exist_ok=True)
    with open(TICKERS_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump({
            'generated_at': datetime.now().isoformat(),
            'tickers': tickers
        }, f, ensure_ascii=False, indent=2)


# ========================================
# ë‹¨ì¼ í‹°ì»¤ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)
# ========================================
def process_single_ticker(args):
    """
    í•œ í‹°ì»¤ì˜ ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì§€í‘œ ê³„ì‚°

    Args:
        args: (market_info, ticker_map, yesterday_date)

    Returns:
        (mcv_id, updated_ticker_data) ë˜ëŠ” None
    """
    market_info, ticker_map, yesterday_date = args
    market = market_info["market"]
    ticker = market.replace("KRW-", "")
    mcv_id = f"{ticker}-KRW-UPBIT"

    # ì–´ì œ ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)
    candle = fetch_yesterday_candle(market)
    if not candle:
        return None

    candle_date = candle["candle_date_time_kst"][:10]

    # ìƒˆ ë ˆì½”ë“œ ìƒì„±
    new_record = {
        'date': candle_date,
        'open': candle["opening_price"],
        'high': candle["high_price"],
        'low': candle["low_price"],
        'close': candle["trade_price"],
        'volume': candle["candle_acc_trade_volume"],
        'rsi': None,
        'ema200_diff': None,
        'ema120_diff': None,
        'ema50_diff': None,
        'ema20_diff': None,
        'volume_ratio_90d': None,
        'volume_ratio_alltime': None
    }

    # ê¸°ì¡´ í‹°ì»¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if mcv_id in ticker_map:
        ticker_data = ticker_map[mcv_id].copy()
    else:
        ticker_data = {
            'mcv_id': mcv_id,
            'ticker': ticker,
            'history': []
        }

    # Upsert: ê°™ì€ ë‚ ì§œ ë®ì–´ì“°ê¸° ë˜ëŠ” ì¶”ê°€
    ticker_data['history'] = upsert_history(ticker_data['history'], new_record)

    # ì§€í‘œ ê³„ì‚° (ìµœê·¼ 250ì¼ ë°ì´í„° ì‚¬ìš©)
    if len(ticker_data['history']) > 0:
        # ë‚ ì§œ ì •ë ¬
        ticker_data['history'].sort(key=lambda x: x['date'])

        # ì§€í‘œ ê³„ì‚°
        indicators = calculate_and_update_indicators(ticker_data['history'])

        # ìµœì‹  ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
        ticker_data['history'][-1].update(indicators)

    return (mcv_id, ticker_data)


# âœ… ë©”ì¸ ì‹¤í–‰
def main():
    print("ğŸš€ ì—…ë¹„íŠ¸ JSON ì—…ë°ì´íŠ¸ ì‹œì‘ (upsert + ë³‘ë ¬ + ì¬ì‹œë„)")

    # 1. ê¸°ì¡´ JSON ë¡œë“œ
    json_data = load_json_data()
    ticker_map = {ticker['mcv_id']: ticker for ticker in json_data['data']}

    print(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(ticker_map)}ê°œ í‹°ì»¤")

    # 2. ì—…ë¹„íŠ¸ ë§ˆì¼“ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    markets = get_krw_markets()
    yesterday_date = (datetime.utcnow() + timedelta(hours=9) - timedelta(days=1)).strftime("%Y-%m-%d")

    print(f"ğŸ“… ì—…ë°ì´íŠ¸ ë‚ ì§œ: {yesterday_date}")
    print(f"ğŸ“‹ ì´ {len(markets)}ê°œ ë§ˆì¼“ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ (max_workers=3)...")

    # 3. ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor, ì¬ì‹œë„ í¬í•¨)
    process_args = [(m, ticker_map, yesterday_date) for m in markets]
    results = parallel_process(
        func=process_single_ticker,
        items=process_args,
        max_workers=3,  # Rate limit íšŒí”¼
        desc="ì—…ë¹„íŠ¸ í‹°ì»¤ ì—…ë°ì´íŠ¸"
    )

    # 4. ê²°ê³¼ ë³‘í•©
    updated_count = 0
    new_tickers = []

    for mcv_id, ticker_data in results:
        is_new = mcv_id not in ticker_map
        ticker_map[mcv_id] = ticker_data
        updated_count += 1

        if is_new:
            new_tickers.append({'mcv_id': mcv_id, 'ticker': ticker_data['ticker']})

    # 3. JSON ì €ì¥
    json_data['data'] = list(ticker_map.values())
    json_data['total_tickers'] = len(ticker_map)
    json_data['total_records'] = sum(len(t['history']) for t in ticker_map.values())
    json_data['generated_at'] = datetime.now().isoformat()

    # âœ… cutoff_dateë¥¼ ì‹¤ì œ ë°ì´í„°ì˜ ìµœì‹  ë‚ ì§œë¡œ ì—…ë°ì´íŠ¸
    all_dates = []
    for ticker_data in json_data['data']:
        for history_entry in ticker_data['history']:
            all_dates.append(history_entry['date'])
    if all_dates:
        json_data['cutoff_date'] = max(all_dates)
        print(f"ğŸ“… cutoff_date ì—…ë°ì´íŠ¸: {json_data['cutoff_date']}")

    save_json_data(json_data)

    # 4. í‹°ì»¤ ëª©ë¡ ì €ì¥
    all_tickers = [{'mcv_id': k, 'ticker': v['ticker']} for k, v in ticker_map.items()]
    save_tickers(all_tickers)

    print(f"\nâœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    print(f"   - ì´ í‹°ì»¤: {len(ticker_map)}")
    print(f"   - ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ: {updated_count}")
    print(f"   - ì‹ ê·œ í‹°ì»¤: {len(new_tickers)}")
    print(f"   - ì´ ë ˆì½”ë“œ: {json_data['total_records']}")

    file_size = os.path.getsize(JSON_FILE_PATH) / 1024 / 1024
    print(f"   - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

if __name__ == "__main__":
    main()
