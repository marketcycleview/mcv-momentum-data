#!/usr/bin/env python3
"""
ë¯¸êµ­ ì£¼ì‹/ETF ì „ì²´ íˆìŠ¤í† ë¦¬ ì¬êµ¬ì¶• (2022-01-01ë¶€í„°)
Yahoo Financeì—ì„œ ë°ì´í„° ê°€ì ¸ì™€ì„œ JSON ìƒì„±

âœ¨ ê°œì„ ì‚¬í•­:
- ë³‘ë ¬ì²˜ë¦¬: ThreadPoolExecutorë¡œ 10ê°œ ë™ì‹œ ì‹¤í–‰ (ê¸°ì¡´)
- ì¬ì‹œë„: API ì‹¤íŒ¨ ì‹œ 4íšŒ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„) - ì‹ ê·œ
"""

import os
import json
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import yfinance as yf
from utils_common import retry_on_failure, calculate_and_update_indicators

# ê²½ë¡œ ì„¤ì •
JSON_FILE_PATH = "src/data/momentum/us/us_historical_data.json"
TICKERS_FILE_PATH = "src/data/momentum/us/us_tickers.json"

# ë¡œì»¬ í‹°ì»¤ íŒŒì¼ ê²½ë¡œ (US í´ë” ì „ì²´)
TICKER_FILES = [
    # Stocks
    "src/data/tickers/us/stocks/stocks_us_nasdaq100_with_mcv_id.json",
    "src/data/tickers/us/stocks/stocks_us_s&p500_with_mcv_id.json",
    "src/data/tickers/us/stocks/stocks_us_russell2000_with_mcv_id.json",  # âœ… Russell 2000 ì¶”ê°€
    # ETF
    "src/data/tickers/us/etf/etf_us_largest_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_leverage_2x_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_leverage_3x_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_others_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_popular_with_mcv_id.json",
    # Index
    "src/data/tickers/us/index/index_us_with_mcv_id.json",
    # Commodity
    "src/data/tickers/us/commodity/commodity_with_mcv_id.json",
    # Bond
    "src/data/tickers/us/bond/bond_us_with_mcv_id.json",
    # Forex
    "src/data/tickers/us/forex/forex_us_with_mcv_id.json",
]

# âœ… ë¡œì»¬ í‹°ì»¤ íŒŒì¼ ë¡œë“œ
def load_local_tickers():
    all_tickers = []
    for file_path in TICKER_FILES:
        if not os.path.exists(file_path):
            print(f"âš ï¸ í‹°ì»¤ íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        with open(file_path, 'r', encoding='utf-8') as f:
            tickers = json.load(f)
            all_tickers.extend(tickers)
            print(f"âœ… ë¡œë“œ: {file_path} - {len(tickers)}ê°œ")

    # ì¤‘ë³µ ì œê±° (mcv_id ê¸°ì¤€)
    unique_tickers = {}
    for t in all_tickers:
        mcv_id = t.get('mcv_id')
        if mcv_id and mcv_id not in unique_tickers:
            unique_tickers[mcv_id] = t

    return list(unique_tickers.values())

# âœ… Yahoo Financeì—ì„œ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬ìš©, ì¬ì‹œë„ ë¡œì§ í¬í•¨)
@retry_on_failure(max_retries=4)
def fetch_yahoo_history(ticker, start_date, end_date):
    """
    Yahoo Financeì—ì„œ OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)
    """
    # yfinanceê°€ ìë™ìœ¼ë¡œ ì„¸ì…˜ ê´€ë¦¬ (curl_cffi ì‚¬ìš©)
    yf_ticker = yf.Ticker(ticker)
    hist = yf_ticker.history(start=start_date, end=end_date)

    if hist.empty:
        return []

    candles = []
    for date, row in hist.iterrows():
        candles.append({
            'date': date.strftime('%Y-%m-%d'),
            'open': round(float(row['Open']), 2) if row['Open'] else None,
            'high': round(float(row['High']), 2) if row['High'] else None,
            'low': round(float(row['Low']), 2) if row['Low'] else None,
            'close': round(float(row['Close']), 2) if row['Close'] else None,
            'volume': int(row['Volume']) if row['Volume'] else 0,
            'rsi': None,
            'ema200_diff': None,
            'ema120_diff': None,
            'ema50_diff': None,
            'ema20_diff': None,
            'volume_ratio_90d': None,
            'volume_ratio_alltime': None
        })

    time.sleep(0.15)  # Rate limit (ì¦ê°€)
    return candles

# âœ… ë‹¨ì¼ í‹°ì»¤ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬í™”ìš©)
def process_single_ticker(t, start_date, end_date, index, total):
    """ë‹¨ì¼ í‹°ì»¤ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰)"""
    ticker = t.get('ticker')
    mcv_id = t.get('mcv_id')
    ko_name = t.get('ko_name')

    if not ticker or not mcv_id:
        return None, f"âš ï¸ [{index}/{total}] ì˜ëª»ëœ í‹°ì»¤ ë°ì´í„°"

    try:
        # Yahoo Financeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        candles = fetch_yahoo_history(ticker, start_date, end_date)

        if len(candles) == 0:
            return None, f"âŒ [{index}/{total}] {ticker} ë°ì´í„° ì—†ìŒ"

        # íˆìŠ¤í† ë¦¬ êµ¬ì¶•
        history = candles

        # ì§€í‘œ ê³„ì‚° (utils_common ì‚¬ìš©)
        if len(candles) > 0:
            indicators = calculate_and_update_indicators(candles)
            candles[-1].update(indicators)

        return {
            'ticker_data': {
                'mcv_id': mcv_id,
                'ticker': ticker,
                'ko_name': ko_name,
                'history': history
            },
            'ticker_info': {
                'mcv_id': mcv_id,
                'ticker': ticker,
                'ko_name': ko_name
            }
        }, f"âœ… [{index}/{total}] {ticker} {len(candles)}ê°œ"

    except Exception as e:
        return None, f"âŒ [{index}/{total}] {ticker} ì—ëŸ¬: {e}"

# âœ… í‹°ì»¤ ëª©ë¡ ì €ì¥
def save_tickers(tickers):
    os.makedirs(os.path.dirname(TICKERS_FILE_PATH), exist_ok=True)
    with open(TICKERS_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump({
            'generated_at': datetime.now().isoformat(),
            'tickers': tickers
        }, f, ensure_ascii=False, indent=2)

# âœ… JSON íŒŒì¼ ì €ì¥
def save_json_data(data):
    os.makedirs(os.path.dirname(JSON_FILE_PATH), exist_ok=True)
    with open(JSON_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# âœ… ë©”ì¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
def main():
    print("ğŸš€ ë¯¸êµ­ ì£¼ì‹/ETF ì „ì²´ íˆìŠ¤í† ë¦¬ ì¬êµ¬ì¶• ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")

    start_date = "2022-01-01"
    end_date = datetime.now().strftime("%Y-%m-%d")

    print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")

    # 1. ë¡œì»¬ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    tickers = load_local_tickers()
    print(f"ğŸ“‹ ì´ {len(tickers)}ê°œ í‹°ì»¤ ì²˜ë¦¬ ì¤‘...\n")
    print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: 5ê°œ ìŠ¤ë ˆë“œ + ì¬ì‹œë„ ë¡œì§ (ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„)\n")

    all_data = []
    all_tickers = []
    failed = 0
    print_lock = Lock()

    # ë³‘ë ¬ ì²˜ë¦¬ (5ê°œ ìŠ¤ë ˆë“œ - Yahoo Finance rate limit íšŒí”¼)
    with ThreadPoolExecutor(max_workers=5) as executor:
        # ëª¨ë“  ì‘ì—…ì„ ì œì¶œ
        future_to_ticker = {
            executor.submit(process_single_ticker, t, start_date, end_date, i+1, len(tickers)): t
            for i, t in enumerate(tickers)
        }

        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for future in as_completed(future_to_ticker):
            result, log_msg = future.result()

            with print_lock:
                print(log_msg)

                if result:
                    all_data.append(result['ticker_data'])
                    all_tickers.append(result['ticker_info'])
                else:
                    failed += 1

                # ì§„í–‰ ìƒí™© (50ê°œë§ˆë‹¤)
                if len(all_data) % 50 == 0:
                    print(f"   ğŸ“Š ì§„í–‰: {len(all_data) + failed}/{len(tickers)} ({(len(all_data) + failed)*100//len(tickers)}%)")

    # JSON ì €ì¥
    json_data = {
        'generated_at': datetime.now().isoformat(),
        'cutoff_date': start_date,
        'total_tickers': len(all_data),
        'total_records': sum(len(t['history']) for t in all_data),
        'data': all_data
    }

    save_json_data(json_data)
    save_tickers(all_tickers)

    print(f"\nâœ… ì¬êµ¬ì¶• ì™„ë£Œ!")
    print(f"   - ì„±ê³µ: {len(all_data)}ê°œ")
    print(f"   - ì‹¤íŒ¨: {failed}ê°œ")
    print(f"   - ì´ ë ˆì½”ë“œ: {json_data['total_records']}")
    print(f"   - ì‹œì‘ì¼: {start_date}")
    print(f"   - ì¢…ë£Œì¼: {end_date}")

    if os.path.exists(JSON_FILE_PATH):
        file_size = os.path.getsize(JSON_FILE_PATH) / 1024 / 1024
        print(f"   - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

if __name__ == "__main__":
    main()
