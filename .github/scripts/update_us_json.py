#!/usr/bin/env python3
"""
ë¯¸êµ­ ì£¼ì‹/ETF ì¼ì¼ ì—…ë°ì´íŠ¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸)
- ê¸°ì¡´ í‹°ì»¤: ì–´ì œ ë°ì´í„°ë§Œ ì¶”ê°€
- ì‹ ê·œ í‹°ì»¤: 2022-01-01ë¶€í„° ì „ì²´ íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ

âœ¨ ê°œì„ ì‚¬í•­:
- upsert: ê°™ì€ ë‚ ì§œ ë°ì´í„° ë®ì–´ì“°ê¸°
- ë³‘ë ¬ì²˜ë¦¬: ThreadPoolExecutorë¡œ 10ê°œ ë™ì‹œ ì‹¤í–‰
- ì¬ì‹œë„: API ì‹¤íŒ¨ ì‹œ 4íšŒ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)
"""

import os
import json
import time
from datetime import datetime, timedelta
import yfinance as yf
from utils_common import retry_on_failure, parallel_process, upsert_history, calculate_and_update_indicators

# ê²½ë¡œ ì„¤ì •
JSON_FILE_PATH = "src/data/momentum/us/us_historical_data.json"
TICKERS_FILE_PATH = "src/data/momentum/us/us_tickers.json"

# ë¡œì»¬ í‹°ì»¤ íŒŒì¼ ê²½ë¡œ (US í´ë” ì „ì²´)
TICKER_FILES = [
    # Stocks
    "src/data/tickers/us/stocks/stocks_us_nasdaq100_with_mcv_id.json",
    "src/data/tickers/us/stocks/stocks_us_s&p500_with_mcv_id.json",
    "src/data/tickers/us/stocks/stocks_us_russell2000_with_mcv_id.json",  # âœ… Russell 2000 ì¶”ê°€
    # ETF
    "src/data/tickers/us/etf/etf_us_largest_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_leverage_2x_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_leverage_3x_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_others_with_mcv_id.json",
    "src/data/tickers/us/etf/etf_us_popular_with_mcv_id.json",
    # Index
    "src/data/tickers/us/index/index_us_with_mcv_id.json",
    # Commodity
    "src/data/tickers/us/commodity/commodity_with_mcv_id.json",
    # Bond
    "src/data/tickers/us/bond/bond_us_with_mcv_id.json",
    # Forex
    "src/data/tickers/us/forex/forex_us_with_mcv_id.json",
]

# ì‹ ê·œ í‹°ì»¤ ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ (rate limit ê³ ë ¤)
MAX_NEW_TICKERS = 5

# âœ… ë¡œì»¬ í‹°ì»¤ íŒŒì¼ ë¡œë“œ
def load_local_tickers():
    all_tickers = []
    for file_path in TICKER_FILES:
        if not os.path.exists(file_path):
            print(f"âš ï¸ í‹°ì»¤ íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        with open(file_path, 'r', encoding='utf-8') as f:
            tickers = json.load(f)
            all_tickers.extend(tickers)

    # ì¤‘ë³µ ì œê±° (mcv_id ê¸°ì¤€)
    unique_tickers = {}
    for t in all_tickers:
        mcv_id = t.get('mcv_id')
        if mcv_id and mcv_id not in unique_tickers:
            unique_tickers[mcv_id] = t

    return list(unique_tickers.values())

# âœ… ê¸°ì¡´ JSON ë°ì´í„° ë¡œë“œ
def load_existing_data():
    if not os.path.exists(JSON_FILE_PATH):
        print("âš ï¸ ê¸°ì¡´ JSON íŒŒì¼ ì—†ìŒ - ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
        return None

    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

# âœ… Yahoo Financeì—ì„œ ìµœê·¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
@retry_on_failure(max_retries=4)
def fetch_yahoo_recent(ticker, start_date, end_date):
    """ë‹¨ì¼ ë‚ ì§œ ë˜ëŠ” ìµœê·¼ ë©°ì¹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)"""
    yf_ticker = yf.Ticker(ticker)
    hist = yf_ticker.history(start=start_date, end=end_date)

    if hist.empty:
        return []

    candles = []
    for date, row in hist.iterrows():
        candles.append({
            'date': date.strftime('%Y-%m-%d'),
            'open': round(float(row['Open']), 2) if row['Open'] else None,
            'high': round(float(row['High']), 2) if row['High'] else None,
            'low': round(float(row['Low']), 2) if row['Low'] else None,
            'close': round(float(row['Close']), 2) if row['Close'] else None,
            'volume': int(row['Volume']) if row['Volume'] else 0,
            'rsi': None,
            'ema200_diff': None,
            'ema120_diff': None,
            'ema50_diff': None,
            'ema20_diff': None,
            'volume_ratio_90d': None,
            'volume_ratio_alltime': None
        })

    time.sleep(0.05)  # Rate limit
    return candles

# âœ… ì „ì²´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì‹ ê·œ í‹°ì»¤ìš©)
@retry_on_failure(max_retries=4)
def fetch_yahoo_full_history(ticker, start_date="2022-01-01"):
    """2022-01-01ë¶€í„° ì „ì²´ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)"""
    end_date = datetime.now().strftime("%Y-%m-%d")
    return fetch_yahoo_recent(ticker, start_date, end_date)

# âœ… JSON íŒŒì¼ ì €ì¥
def save_json_data(data):
    os.makedirs(os.path.dirname(JSON_FILE_PATH), exist_ok=True)
    with open(JSON_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ========================================
# ê¸°ì¡´ í‹°ì»¤ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)
# ========================================
def process_existing_ticker(args):
    """
    ê¸°ì¡´ í‹°ì»¤ì˜ ì–´ì œ ë°ì´í„° ì¶”ê°€ (upsert)

    Args:
        args: (ticker_info, existing_ticker_data, yesterday)

    Returns:
        (mcv_id, updated_data) ë˜ëŠ” None
    """
    ticker_info, ticker_data, yesterday = args
    ticker = ticker_info['ticker']
    mcv_id = ticker_info['mcv_id']

    # ì–´ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)
    candles = fetch_yahoo_recent(ticker, yesterday, yesterday)

    if len(candles) == 0:
        return None  # ì£¼ë§/íœ´ì¼

    # Upsert: ê°™ì€ ë‚ ì§œ ë®ì–´ì“°ê¸°
    for candle in candles:
        ticker_data['history'] = upsert_history(ticker_data['history'], candle)

    # ë‚ ì§œ ì •ë ¬
    ticker_data['history'].sort(key=lambda x: x['date'])

    # ì§€í‘œ ì¬ê³„ì‚°
    if len(ticker_data['history']) > 0:
        indicators = calculate_and_update_indicators(ticker_data['history'])
        ticker_data['history'][-1].update(indicators)

    return (mcv_id, ticker_data)


# ========================================
# ì‹ ê·œ í‹°ì»¤ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)
# ========================================
def process_new_ticker(ticker_info):
    """
    ì‹ ê·œ í‹°ì»¤ì˜ ì „ì²´ íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ

    Args:
        ticker_info: í‹°ì»¤ ì •ë³´ ë”•ì…”ë„ˆë¦¬

    Returns:
        ticker_data ë˜ëŠ” None
    """
    ticker = ticker_info['ticker']
    mcv_id = ticker_info['mcv_id']
    ko_name = ticker_info.get('ko_name')

    # ì „ì²´ íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ (ì¬ì‹œë„ í¬í•¨)
    candles = fetch_yahoo_full_history(ticker)

    if len(candles) == 0:
        return None

    # ì§€í‘œ ê³„ì‚°
    if len(candles) > 0:
        indicators = calculate_and_update_indicators(candles)
        candles[-1].update(indicators)

    return {
        'mcv_id': mcv_id,
        'ticker': ticker,
        'ko_name': ko_name,
        'history': candles
    }


# âœ… ë©”ì¸ ì‹¤í–‰
def main():
    print("ğŸ”„ ë¯¸êµ­ ì£¼ì‹/ETF ì¼ì¼ ì—…ë°ì´íŠ¸ ì‹œì‘ (upsert + ë³‘ë ¬ + ì¬ì‹œë„)...")

    # ì–´ì œ ë‚ ì§œ ê³„ì‚° (ë¯¸êµ­ ì‹œì¥ ê¸°ì¤€)
    yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    print(f"ğŸ“… ì—…ë°ì´íŠ¸ ë‚ ì§œ: {yesterday}\n")

    # 1. í‹°ì»¤ ì†ŒìŠ¤ ë¡œë“œ
    local_tickers = load_local_tickers()
    print(f"ğŸ“‹ ë¡œì»¬ í‹°ì»¤: {len(local_tickers)}ê°œ")

    # TODO: Watchlist í‹°ì»¤ ê°€ì ¸ì˜¤ê¸° (DB ì—°ë™)
    # watchlist_tickers = get_watchlist_tickers()
    # all_tickers = merge_unique(local_tickers, watchlist_tickers)
    all_tickers = local_tickers  # í˜„ì¬ëŠ” ë¡œì»¬ë§Œ

    # 2. ê¸°ì¡´ JSON ë°ì´í„° ë¡œë“œ
    existing_data = load_existing_data()
    if not existing_data:
        print("âŒ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - rebuild_us_history.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")
        return

    # ê¸°ì¡´ í‹°ì»¤ ë§µ ìƒì„±
    existing_map = {item['mcv_id']: item for item in existing_data['data']}
    existing_mcv_ids = set(existing_map.keys())

    # 3. ì‹ ê·œ í‹°ì»¤ vs ê¸°ì¡´ í‹°ì»¤ ë¶„ë¦¬
    new_tickers = [t for t in all_tickers if t['mcv_id'] not in existing_mcv_ids]
    existing_tickers = [t for t in all_tickers if t['mcv_id'] in existing_mcv_ids]

    print(f"ğŸ†• ì‹ ê·œ í‹°ì»¤: {len(new_tickers)}ê°œ")
    print(f"ğŸ”„ ê¸°ì¡´ í‹°ì»¤: {len(existing_tickers)}ê°œ\n")

    # 4. ê¸°ì¡´ í‹°ì»¤ ì—…ë°ì´íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬ + upsert)
    print(f"ğŸ“Š ê¸°ì¡´ í‹°ì»¤ ì—…ë°ì´íŠ¸ ì¤‘ (max_workers=10)...")
    process_args = [(t, existing_map[t['mcv_id']].copy(), yesterday) for t in existing_tickers]
    results = parallel_process(
        func=process_existing_ticker,
        items=process_args,
        max_workers=10,
        desc="ê¸°ì¡´ í‹°ì»¤ ì—…ë°ì´íŠ¸"
    )

    # ê²°ê³¼ ë³‘í•©
    for mcv_id, updated_data in results:
        existing_map[mcv_id] = updated_data

    print(f"âœ… ê¸°ì¡´ í‹°ì»¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(results)}ê°œ\n")

    # 5. ì‹ ê·œ í‹°ì»¤ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)
    if len(new_tickers) > 0:
        # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì²˜ë¦¬ (ë‚˜ë¨¸ì§€ëŠ” ë‹¤ìŒë‚ )
        if len(new_tickers) > MAX_NEW_TICKERS:
            print(f"âš ï¸ ì‹ ê·œ í‹°ì»¤ {len(new_tickers)}ê°œ ê°ì§€ - ì˜¤ëŠ˜ì€ {MAX_NEW_TICKERS}ê°œë§Œ ì²˜ë¦¬")
            process_new = new_tickers[:MAX_NEW_TICKERS]
        else:
            process_new = new_tickers

        print(f"ğŸ†• ì‹ ê·œ í‹°ì»¤ ì²˜ë¦¬ ì¤‘ (max_workers=10)...")
        new_results = parallel_process(
            func=process_new_ticker,
            items=process_new,
            max_workers=10,
            desc="ì‹ ê·œ í‹°ì»¤ ë‹¤ìš´ë¡œë“œ"
        )

        # ê²°ê³¼ ì¶”ê°€
        for ticker_data in new_results:
            existing_data['data'].append(ticker_data)

        print(f"âœ… ì‹ ê·œ í‹°ì»¤ ì²˜ë¦¬ ì™„ë£Œ: {len(new_results)}ê°œ\n")

    # 6. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    existing_data['generated_at'] = datetime.now().isoformat()
    existing_data['total_tickers'] = len(existing_data['data'])
    existing_data['total_records'] = sum(len(t['history']) for t in existing_data['data'])

    # 7. JSON ì €ì¥
    save_json_data(existing_data)

    print("âœ… ì¼ì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    print(f"   - ì´ í‹°ì»¤: {existing_data['total_tickers']}ê°œ")
    print(f"   - ì´ ë ˆì½”ë“œ: {existing_data['total_records']}")
    print(f"   - ì—…ë°ì´íŠ¸ ì‹œê°: {existing_data['generated_at']}")

    if os.path.exists(JSON_FILE_PATH):
        file_size = os.path.getsize(JSON_FILE_PATH) / 1024 / 1024
        print(f"   - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")

if __name__ == "__main__":
    main()
